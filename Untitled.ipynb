{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required packages for this module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "#Loading the data\n",
    "data0 = pd.read_csv('E:/New folder (2)/OneDrive - Amrita Vishwa Vidyapeetham/Attachments/bh_dataset/5.urldata.csv')\n",
    "data0.head()\n",
    "\n",
    "#Checking the shape of the dataset\n",
    "data0.shape\n",
    "\n",
    "#Listing the features of the dataset\n",
    "data0.columns\n",
    "\n",
    "#Information about the dataset\n",
    "data0.info()\n",
    "\n",
    "data0.describe()\n",
    "\n",
    "data0.dtypes\n",
    "\n",
    "data = data0.drop(['Domain'], axis = 1).copy()\n",
    "#Correlation heatmap\n",
    "\n",
    "plt.figure(figsize=(15,13))\n",
    "sns.heatmap(data.corr(),annot=True)\n",
    "plt.show()\n",
    "#Dropping the Domain column\n",
    "\n",
    "#checking the data for null or missing values\n",
    "data.isnull().sum()\n",
    "\n",
    "# shuffling the rows in the dataset so that when splitting the train and test set are equally distributed\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head()\n",
    "\n",
    "# Sepratating & assigning features and target columns to X & y\n",
    "y = data['Label']\n",
    "X = data.drop('Label',axis=1)\n",
    "X.shape, y.shape\n",
    "\n",
    "# Splitting the dataset into train and test sets: 80-20 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 12)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "#importing packages\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ML_Model = []\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "#function to call for storing the results\n",
    "def storeResults(model, a,b):\n",
    "  ML_Model.append(model)\n",
    "  acc_train.append(round(a, 3))\n",
    "  acc_test.append(round(b, 3))\n",
    "\n",
    "# Decision Tree model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# instantiate the model \n",
    "tree = DecisionTreeClassifier(max_depth = 5)\n",
    "# fit the model \n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "#predicting the target value from the model for the samples\n",
    "y_test_tree = tree.predict(X_test)\n",
    "y_train_tree = tree.predict(X_train)\n",
    "\n",
    "#computing the accuracy of the model performance\n",
    "acc_train_tree = accuracy_score(y_train,y_train_tree)\n",
    "acc_test_tree = accuracy_score(y_test,y_test_tree)\n",
    "sns.heatmap(confusion_matrix(y_test,y_test_tree),annot= True,fmt='g').set_title('Confussion Matrix')\n",
    "\n",
    "print(classification_report(y_test,y_test_tree))\n",
    "\n",
    "print(\"Decision Tree: Accuracy on training Data:\",acc_train_tree)\n",
    "\n",
    "print(\"Decision Tree: Accuracy on test Data:\",acc_test_tree)\n",
    "\n",
    "#checking the feature improtance in the model\n",
    "plt.figure(figsize=(9,7))\n",
    "n_features = X_train.shape[1]\n",
    "plt.barh(range(n_features), tree.feature_importances_, align='center')\n",
    "plt.yticks(np.arange(n_features), X_train.columns)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "#navie bayees classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "GNBmodel=GaussianNB()\n",
    "GNBmodel.fit(X_train, y_train)\n",
    "\n",
    "GNBpredicted = GNBmodel.predict(X_test)\n",
    "\n",
    "GNBacc=accuracy_score(y_test,GNBpredicted)\n",
    "sns.heatmap(confusion_matrix(y_test,GNBpredicted),annot= True,fmt='g').set_title('Confussion Matrix')\n",
    "print(classification_report(y_test,GNBpredicted))\n",
    "\n",
    "print(\"Naive bayees: Accuracy on test Data:\",GNBacc)\n",
    "\n",
    "# Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate the model\n",
    "forest = RandomForestClassifier(max_depth=5)\n",
    "\n",
    "# fit the model \n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "#predicting the target value from the model for the samples\n",
    "y_test_forest = forest.predict(X_test)\n",
    "y_train_forest = forest.predict(X_train)\n",
    "\n",
    "#computing the accuracy of the model performance\n",
    "acc_train_forest = accuracy_score(y_train,y_train_forest)\n",
    "acc_test_forest = accuracy_score(y_test,y_test_forest)\n",
    "sns.heatmap(confusion_matrix(y_test,y_test_forest),annot= True,fmt='g').set_title('Confussion Matrix')\n",
    "print(classification_report(y_test,y_test_forest))\n",
    "print(\"Random forest: Accuracy on training Data: {:.3f}\".format(acc_train_forest))\n",
    "print(\"Random forest: Accuracy on test Data: {:.3f}\".format(acc_test_forest))\n",
    "\n",
    "#checking the feature improtance in the model\n",
    "plt.figure(figsize=(9,7))\n",
    "n_features = X_train.shape[1]\n",
    "plt.barh(range(n_features), forest.feature_importances_, align='center')\n",
    "plt.yticks(np.arange(n_features), X_train.columns)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "     \n",
    "\n",
    "\n",
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "#Caution: Execute only once to avoid duplications.\n",
    "storeResults('Random Forest', acc_train_forest, acc_test_forest)\n",
    "\n",
    "# Multilayer Perceptrons model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# instantiate the model\n",
    "mlp = MLPClassifier(alpha=0.001, hidden_layer_sizes=([100,100,100]))\n",
    "\n",
    "# fit the model \n",
    "mlp.fit(X_train, y_train)\n",
    "#predicting the target value from the model for the samples\n",
    "y_test_mlp = mlp.predict(X_test)\n",
    "y_train_mlp = mlp.predict(X_train)\n",
    "\n",
    "#computing the accuracy of the model performance\n",
    "acc_train_mlp = accuracy_score(y_train,y_train_mlp)\n",
    "acc_test_mlp = accuracy_score(y_test,y_test_mlp)\n",
    "sns.heatmap(confusion_matrix(y_test,y_test_mlp),annot= True,fmt='g').set_title('Confussion Matrix')\n",
    "print(classification_report(y_test,y_test_mlp))\n",
    "print(\"Multilayer Perceptrons: Accuracy on training Data: {:.3f}\".format(acc_train_mlp))\n",
    "print(\"Multilayer Perceptrons: Accuracy on test Data: {:.3f}\".format(acc_test_mlp))\n",
    "\n",
    "#! pip install xgboost\n",
    "\n",
    "#XGBoost Classification model\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# instantiate the model\n",
    "xgb = XGBClassifier(learning_rate=0.4,max_depth=7)\n",
    "#fit the model\n",
    "xgb.fit(X_train, y_train)\n",
    "#predicting the target value from the model for the samples\n",
    "y_test_xgb = xgb.predict(X_test)\n",
    "y_train_xgb = xgb.predict(X_train)\n",
    "#computing the accuracy of the model performance\n",
    "acc_train_xgb = accuracy_score(y_train,y_train_xgb)\n",
    "acc_test_xgb = accuracy_score(y_test,y_test_xgb)\n",
    "sns.heatmap(confusion_matrix(y_test,y_test_xgb),annot= True,fmt='g').set_title('Confussion Matrix')\n",
    "print(classification_report(y_test,y_test_xgb))\n",
    "print(\"XGBoost: Accuracy on training Data: {:.3f}\".format(acc_train_xgb))\n",
    "print(\"XGBoost : Accuracy on test Data: {:.3f}\".format(acc_test_xgb))\n",
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "#Caution: Execute only once to avoid duplications.\n",
    "storeResults('XGBoost', acc_train_xgb, acc_test_xgb)\n",
    "\n",
    "#! pip install keras \n",
    "\n",
    "\n",
    "\n",
    "#pip install tensorflow\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn import metrics\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from sklearn import metrics\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = input_dim\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"relu\",\n",
    "                activity_regularizer=regularizers.l1(10e-4))(input_layer)\n",
    "encoder = Dense(int(encoding_dim), activation=\"relu\")(encoder)\n",
    "\n",
    "encoder = Dense(int(encoding_dim-2), activation=\"relu\")(encoder)\n",
    "code = Dense(int(encoding_dim-4), activation='relu')(encoder)\n",
    "decoder = Dense(int(encoding_dim-2), activation='relu')(code)\n",
    "\n",
    "decoder = Dense(int(encoding_dim), activation='relu')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.summary()\n",
    "\n",
    "#compiling the model\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "#Training the model\n",
    "history = autoencoder.fit(X_train, X_train, epochs=4, batch_size=64, shuffle=True, validation_split=0.2) \n",
    "\n",
    "acc_train_auto = autoencoder.evaluate(X_train, X_train)[1]\n",
    "acc_test_auto = autoencoder.evaluate(X_test, X_test)[1]\n",
    "\n",
    "print('\\nAutoencoder: Accuracy on training Data: {:.3f}' .format(acc_train_auto))\n",
    "print('Autoencoder: Accuracy on test Data: {:.3f}' .format(acc_test_auto))\n",
    "\n",
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "#Caution: Execute only once to avoid duplications.\n",
    "storeResults('AutoEncoder', acc_train_auto, acc_test_auto)\n",
    "\n",
    "#Support vector machine model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# instantiate the model\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=12)\n",
    "#fit the model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#predicting the target value from the model for the samples\n",
    "y_test_svm = svm.predict(X_test)\n",
    "y_train_svm = svm.predict(X_train)\n",
    "#computing the accuracy of the model performance\n",
    "acc_train_svm = accuracy_score(y_train,y_train_svm)\n",
    "acc_test_svm = accuracy_score(y_test,y_test_svm)\n",
    "\n",
    "\n",
    "print(\"SVM: Accuracy on training Data: {:.3f}\".format(acc_train_svm))\n",
    "print(\"SVM : Accuracy on test Data: {:.3f}\".format(acc_test_svm))\n",
    "\n",
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "#Caution: Execute only once to avoid duplications.\n",
    "storeResults('SVM', acc_train_svm, acc_test_svm)\n",
    "\n",
    "#creating dataframe\n",
    "results = pd.DataFrame({ 'ML Model': ML_Model,    \n",
    "    'Train Accuracy': acc_train,\n",
    "    'Test Accuracy': acc_test})\n",
    "results\n",
    "\n",
    "#Sorting the datafram on accuracy\n",
    "results.sort_values(by=['Test Accuracy', 'Train Accuracy'], ascending=False)\n",
    "\n",
    "# save XGBoost model to file\n",
    "import pickle\n",
    "pickle.dump(xgb, open(\"XGBoostClassifier.pickle.dat\", \"wb\"))\n",
    "\n",
    "# load model from file\n",
    "loaded_model = pickle.load(open(\"XGBoostClassifier.pickle.dat\", \"rb\"))\n",
    "loaded_model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
